set_engine("MASS")
qda_mod<-discrim_quad() %>%
set_mode("classification") %>%
set_engine("MASS")
knn_mod<-nearest_neighbor() %>%
set_mode("classification") %>%
set_engine("kknn")
svm_linear_mod<- svm_linear() %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_rad_mod<-svm_rbf %>%
set_mode("classification") %>%
set_engine("MASS")
library(discrim)
lda_mod<-discrim_linear() %>%
set_mode("classification") %>%
set_engine("MASS")
qda_mod<-discrim_quad() %>%
set_mode("classification") %>%
set_engine("MASS")
knn_mod<-nearest_neighbor() %>%
set_mode("classification") %>%
set_engine("kknn")
svm_linear_mod<- svm_linear() %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_rad_mod<-svm_rbf %>%
set_mode("classification") %>%
set_engine("kernlab")
library(discrim)
lda_mod<-discrim_linear() %>%
set_mode("classification") %>%
set_engine("MASS")
qda_mod<-discrim_quad() %>%
set_mode("classification") %>%
set_engine("MASS")
knn_mod<-nearest_neighbor() %>%
set_mode("classification") %>%
set_engine("kknn")
svm_linear_mod<- svm_linear() %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_rad_mod<-svm_rbf() %>%
set_mode("classification") %>%
set_engine("kernlab")
library(discrim)
lda_mod<-discrim_linear() %>%
set_mode("classification") %>%
set_engine("MASS")
qda_mod<-discrim_quad() %>%
set_mode("classification") %>%
set_engine("MASS")
knn_mod<-nearest_neighbor() %>%
set_mode("classification") %>%
set_engine("kknn") %>%
set_args(neighbors=tune())
svm_linear_mod<- svm_linear() %>%
set_mode("classification") %>%
set_engine("kernlab") %>%
set_args(cost=tune())
svm_rad_mod<-svm_rbf() %>%
set_mode("classification") %>%
set_engine("kernlab") %>%
set_args(cost=tune(), rbf_sigma=tune())
#Quand on est en tidyverse on ne met plus des points pour creer des objets, on met des tirets du bas _
dat_rec<- recipe(AHDÃge+RestBP+
Chol + MaxHR +
Oldpeak + Ca,
data = dat_train) #On a mis que des variables quantitatives pour etre sur que ça tourne sans problèmes
#Quand on est en tidyverse on ne met plus des points pour creer des objets, on met des tirets du bas _
dat_rec<- recipe(AHD~ Age+RestBP+
Chol + MaxHR +
Oldpeak + Ca,
data = dat_train) #On a mis que des variables quantitatives pour etre sur que ça tourne sans problèmes
lda_wf <- workflow() %>%
add_model(lda_mod) %>%
add_recipe(dat_rec)
qda_wf<- workflow() %>%
add_model(qda_mod) %>%
add_recipe(dat_rec)
knn_wf<- workflow() %>%
add_model(knn_mod) %>%
add_recipe(dat_rec)
svm_lin_wf<- workflow() %>%
add_model(svm_linear_mod) %>%
add_recipe(dat_rec)
svm_rad_wf<- workflow() %>%
add_model(svm_rad_mod) %>%
add_recipe(dat_rec)
df_folds<-vfold_cv(training(split_dat), v=5, strata=AHD)
knn_grid<- grid_regular(neighbors(), levels=10)
tune_res_knn<- tune_grid(knn_wf,
resamples=df_folds,
grid=knn_grid)
knn_grid<-grid_regular(neighbors(), levels=10)
tune_res_knn<- tune_grid(knn_wf,
resamples=df_folds,
grid=knn_grid)
install.packages("kknn")
library(discrim)
library(kknn)
lda_mod<-discrim_linear() %>%
set_mode("classification") %>%
set_engine("MASS")
qda_mod<-discrim_quad() %>%
set_mode("classification") %>%
set_engine("MASS")
knn_mod<-nearest_neighbor() %>%
set_mode("classification") %>%
set_engine("kknn") %>%
set_args(neighbors=tune())
svm_linear_mod<- svm_linear() %>%
set_mode("classification") %>%
set_engine("kernlab") %>%
set_args(cost=tune())
svm_rad_mod<-svm_rbf() %>%
set_mode("classification") %>%
set_engine("kernlab") %>%
set_args(cost=tune(), rbf_sigma=tune())
#Quand on est en tidyverse on ne met plus des points pour creer des objets, on met des tirets du bas _
dat_rec<- recipe(AHD~ Age+RestBP+
Chol + MaxHR +
Oldpeak + Ca,
data = dat_train) #On a mis que des variables quantitatives pour etre sur que ça tourne sans problèmes
lda_wf <- workflow() %>%
add_model(lda_mod) %>%
add_recipe(dat_rec)
qda_wf<- workflow() %>%
add_model(qda_mod) %>%
add_recipe(dat_rec)
knn_wf<- workflow() %>%
add_model(knn_mod) %>%
add_recipe(dat_rec)
svm_lin_wf<- workflow() %>%
add_model(svm_linear_mod) %>%
add_recipe(dat_rec)
svm_rad_wf<- workflow() %>%
add_model(svm_rad_mod) %>%
add_recipe(dat_rec)
df_folds<-vfold_cv(training(split_dat), v=5, strata=AHD)
knn_grid<-grid_regular(neighbors(), levels=10)
tune_res_knn<- tune_grid(knn_wf,
resamples=df_folds,
grid=knn_grid)
autoplot(tune_res_knn)
knn_best<-tune_res_knn %>% select_best(metric="accuracy")#on aurait pu prendre la metric roc.
knn_final_wk<-knnwf %>% finlize_workflows(knn_best)
knn_final_wf<-knnwf %>% finlize_workflows(knn_best)
knn_final_wf<-knnwf %>% finalize_workflows(knn_best)
knn_final_wf<-knnwf %>% finalize_workflo(knn_best)
knn_final_wf<-knnwf %>% finalize_workflow(knn_best)
knn_final_wf<-knn_wf %>% finalize_workflow(knn_best)
18000*3
18000*30
18000*90
18000*134.99
12000*134.99
30*12000
16*1700*12
16*1700*12*1.4
16*1700*12/0.6
12*2200/0.6
12*5*2200/0.6
a<-16*1700*12/0.6
b<-12*2200/0.6
c<-12*5*2200/0.6
a+b+c
c<-12*5*2200/1.4
b<-12*2200/1.4
a<-16*1700*12/1.4
a+b+c
a<-16*1700*12*1.4
b<-12*2200*1.4
c<-12*5*2200*1.4
a+b+c
d<- 4000*12*1.4
e<- 1500*12*1.4
a+b+c
a+b+c+d+e
(8*1500)/2
24000*200
0.15*24000
# Cas pratique 2 partie 2
200*24000
400000 + 200000 + 600000 + 306402*4
6*8
48*1200/5
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rsample)
data <- iris
set.seed(73)
#création du découpage
data_split <- data %>% initial_split(prop = 2/3)
#création des sous-échantillons apprentissage-test
test_data <- data_split %>% testing()
train_data <- data_split %>% training()
library(rpart) # package pour les arbres CART
library(rpart) # package pour les arbres CART
control.max <- rpart.control(cp = 0, max.depth = 0, minbucket = 1, minsplit = 1)
tree <- rpart(Species~. , data = train_data, control = control.max,
parms = list(split = "information"))
library(rpart) # package pour les arbres CART
control.max <- rpart.control(cp = 0, max.depth = 0, minbucket = 1, minsplit = 1)
tree <- rpart(Species~. , data = train_data, control = control.max,
parms = list(split = "information"))
plot(tree)
text(tree)
tree
summary(tree)
?rpart.control
pred.tree <- predict(tree, newdata = train_data, type = "class")
table(pred.tree, train_data[,5])
mean(pred.tree != train_data[,5])
pred.tree <- predict(tree, newdata = test_data, type = "class")
table(pred.tree, test_data[,5])
mean(pred.tree != test_data[,5])
plotcp(tree)
tree$cptable
tree
library(rpart.plot)
prp(tree, type = 0, extra = 0, split.box.col = "lightblue", cex = 0.6)
prp(tree, type = 1, extra = 1, split.box.col = "lightblue", cex = 0.6)
plotcp(tree)
tree$cptable
sqrt(0.390625*0.01562500)
tree$cptable[which.min(tree$cptable[,4]),1]
# cp_choisie à définir
treebis <- prune(tree, cp = cp_choisie) # en élaguant un arbre existant
cp_choisie<-tree$cptable[which.min(tree$cptable[,4]),1]
# cp_choisie à définir
treebis <- prune(tree, cp = cp_choisie) # en élaguant un arbre existant
control.prune <- rpart.control(cp = cp_choisie, max.depth = 0, minbucket = 1, minsplit = 1)
treeter <- rpart(Species~. , data = train_data, control = control.prune,
parms = list(split = "information"))
tree$variable.importance
treebis$variable.importance
cp_choisie<-sqrt(0.390625*0.01562500)
tree$cptable[which.min(tree$cptable[,4]),1]
# cp_choisie à définir
treebis <- prune(tree, cp = cp_choisie) # en élaguant un arbre existant
control.prune <- rpart.control(cp = cp_choisie, max.depth = 0, minbucket = 1, minsplit = 1)
treeter <- rpart(Species~. , data = train_data, control = control.prune,
parms = list(split = "information"))
tree$variable.importance
treebis$variable.importance
plot(tree)
plotcp(treebis)
tree$cptable
plotcp(treeter)
plotcp(treebis)
tree$cptable
plotcp(treebis)
plotcp(treebis)
tree$cptable
tree$variable.importance
treebis$variable.importance
library(tidymodels)
barplot(tree$variable.importance)
barplot(tree$variable.importance, col = iris$Species)
barplot(tree$variable.importance, col = rainbow(4))
barplot(tree$variable.importance)
data.frame(vi = tree$variable.importance,
variable = names(tree$variable.importance)) %>%
ggplot() + aes(x= reorder(variable, vi), y = vi) +
geom_col(width = 0.4, color = "aquamarine3", fill = "aquamarine3") +
coord_flid() + labs(y = "Mesure d'importance", x = NULL)
data.frame(vi = tree$variable.importance,
variable = names(tree$variable.importance)) %>%
ggplot() + aes(x= reorder(variable, vi), y = vi) +
geom_col(width = 0.4, color = "aquamarine3", fill = "aquamarine3") +
coord_flip() + labs(y = "Mesure d'importance", x = NULL)
data.frame(vi = tree$variable.importance,
variable = names(tree$variable.importance)) %>%
ggplot() + aes(x= reorder(variable, vi), y = vi) +
geom_col(width = 0.4, color = "cornsilk3", fill = "cornsilk3") +
coord_flip() + labs(y = "Mesure d'importance", x = NULL)
data.frame(vi = tree$variable.importance,
variable = names(tree$variable.importance)) %>%
ggplot() + aes(x= reorder(variable, vi), y = vi) +
geom_segment(aes(x= reorder(variable, vi),
xend = reorder(variable, vi),
y = 0,
yend = vi
),
col = "gray") +
geom_point(color = "aquamarine3", size = 3) #+
#  coord_flip() + labs(y = "Mesure d'importance", x = NULL)
data.frame(vi = tree$variable.importance,
variable = names(tree$variable.importance)) %>%
ggplot() + aes(x= reorder(variable, vi), y = vi) +
geom_segment(aes(x= reorder(variable, vi),
xend = reorder(variable, vi),
y = 0,
yend = vi
),
col = "gray") +
geom_point(color = "aquamarine3", size = 3) +
coord_flip() + labs(y = "Mesure d'importance", x = NULL)
set.seed(73)
split_data <- initial_split(data, prop = 0.75, strata = AHD)
library(tidymodels)
library(readr)
data <- read_csv("Heart.csv")[, -1] %>% tidyr::drop_na()
data <- data %>%
mutate(
AHD = factor(AHD),
Sex = as.logical(Sex),
ChestPain = factor(ChestPain),
Fbs = as.logical(Fbs),
RestECG = factor(RestECG),
ExAng = factor(ExAng),
Slope = factor(Slope, ordered = TRUE, levels = c(1, 2, 3)),
Thal = factor(Thal)
)
set.seed(73)
split_data <- initial_split(data, prop = 0.75, strata = AHD)
data_train <- training(split_data)
data_test <- testing(split_data)
rec <- recipe(AHD ~ ., data = data_train)
tree_spec <- decision_tree() %>%
set_engine("rpart") %>%
set_mode("classification")
# workflow avec paramètres à choisir
tune_tree_wf <- workflow() %>%
add_model(tree_spec %>%
set_args(cost_complexity = tune())
) %>%
add_recipe(rec)
# création des échantillons de validation croisée
data_cv <- vfold_cv(data_train)
# valeurs à tester
cost_complexity_grid <- grid_regular(cost_complexity(range = c(-5,-0.1)),
levels = 15)
#
tree_tune_res <- tune_grid(
tune_tree_wf, # le workflow
resamples = data_cv, # les échantillons de validation croisée
grid = cost_complexity_grid, # la grille des valeurs à tester
metrics = metric_set(accuracy) # la métrique pour choisir la meilleur valeur
)
autoplot(tree_tune_res)
tree_tune_res %>% show_best("accuracy")
best_cost_complexity <- select_best(tree_tune_res)
final_tune_tree_wf <- tune_tree_wf %>%
finalize_workflow(best_cost_complexity)
autoplot(tree_tune_res)
tree_tune_res %>% show_best("accuracy")
best_cost_complexity <- select_best(tree_tune_res)
final_tune_tree_wf <- tune_tree_wf %>%
finalize_workflow(best_cost_complexity)
autoplot(tree_tune_res)
final_tune_tree_wf <- tune_tree_wf %>%
finalize_workflow(best_cost_complexity)
final_tune_tree_wf
best_cost_complexity
tree_fit <- final_tune_tree_wf %>% last_fit(split_data)
# construit le modele sur les donnes d'apprentissage et fait les predictions sur les donnees test, on a plus qu'à collleccter les données.
tree_fit %>% collect_metrics()
tree_fit %>% collect_predictions()
tree_fit %>% collect_predictions() %>% roc_curve(AHD, .pred_No) %>% autoplot()
tree_fit %>%
extract_fit_engine() %>%
rpart.plot::prp(type = 0, extra = 1, split.box.col = "lightblue",
roundint = FALSE)
final_model <-  final_tune_tree_wf %>%
fit(data) # données complètes
save(final_model, file = "mymodel.RData")
load("C:/Users/karl/OneDrive/Documents/Master 1/Semestre 2/Data Mining/mymodel.RData")
save(final_model, file = "mymodel.RData")
a_predire <- data.frame(
Age = c(50, 60, 70),
Sex = c( TRUE, FALSE, FALSE),
ChestPain = c("nonanginal", "typical", "asymptomatic"),
RestBP = c(150, 118, NA),
Fbs = c(FALSE, FALSE, FALSE),
Chol = c(350, 280, 200),
RestECG = c("1", "2", "2"),
MaxHR = c(172, 160, 125),
ExAng = c("0", "0", "1"),
Oldpeak = c(3, 0, 1),
Slope = c("2", "1", "1"),
Ca = c(3, 0, 1),
Thal = c("reversable", "normal", "normal")
)
a_predire <- data.frame(
Age = c(50, 60, 70),
Sex = c( TRUE, FALSE, FALSE),
ChestPain = c("nonanginal", "typical", "asymptomatic"),
RestBP = c(150, 118, NA),
Fbs = c(FALSE, FALSE, FALSE),
Chol = c(350, 280, 200),
RestECG = c("1", "2", "2"),
MaxHR = c(172, 160, 125),
ExAng = c("0", "0", "1"),
Oldpeak = c(3, 0, 1),
Slope = c("2", "1", "1"),
Ca = c(3, 0, 1),
Thal = c("reversable", "normal", "normal")
)
# chargement du modèle
load("mymodel.RData")
# cette commande créera un objet du même nom qu'au départ
predict(final_model, new_data = a_predire) # par défaut type = "class"
predict(final_model, new_data = a_predire, type = "prob")
predict(final_model, new_data = a_predire, type = "raw") # change juste le format de sortie
data <- read_csv("Heart.csv")[, -1] %>% tidyr::drop_na()
data <- data %>%
mutate(
AHD = factor(AHD),
Sex = as.logical(Sex),
ChestPain = factor(ChestPain),
Fbs = as.logical(Fbs),
RestECG = factor(RestECG),
ExAng = factor(ExAng),
Slope = factor(Slope, ordered = TRUE, levels = c(1, 2, 3)),
Thal = factor(Thal)
)
30000000/50000
1100*600*50000
400*400*50000
100000000-50000*1400
100000000-50000*1600
setwd("~/Master 1/Semestre 2/Data Mining/Projet Bank Marketind Data")
knitr::opts_chunk$set(echo = TRUE)
data<- read.csv("train.csv", header = TRUE, sep = ";")
head(data)
str(data)
summary(data)
View(data)
rowSums(is.na(data))
colSums(is.na(data))
library(visdat)
par(mfrow = c(1,2))
vis_dat(data, palette = "default")
vis_dat(data, palette = "default")
gg_miss_var(coffee,show_pct = TRUE)
gg_miss_var(data,show_pct = TRUE)
library(FactoMineR)
library(factoextra)
library(graphics)
library(tidyverse)
library(ggplot2)
library(knitr)
library(corrplot)
library(tinytex)
library(kableExtra)
library(dplyr)
library(ggcorrplot)
library(ggpubr)
library(DT)
library(ggpubr)
library(questionr)
library(ggcorrplot)
library(tikzDevice)
library(cluster)
library(ggthemes)
library(WeightedCluster)
library(patchwork)
library(tinytex)
library(FactoMineR)
library(factoextra)
library(graphics)
library(tidyverse)
library(ggplot2)
library(knitr)
library(corrplot)
library(tinytex)
library(kableExtra)
library(dplyr)
library(ggcorrplot)
library(ggpubr)
library(DT)
library(ggpubr)
library(questionr)
library(ggcorrplot)
library(tikzDevice)
library(cluster)
library(ggthemes)
library(WeightedCluster)
library(patchwork)
library(tinytex)
library(visdat)
par(mfrow = c(1,2))
vis_dat(data, palette = "default")
gg_miss_var(data,show_pct = TRUE)
gg_miss_var(data,show_pct = TRUE)
vis_dat(data, palette = "default")
length(data[,:])
length(data[,1:17])
length(data[,1])
length(col(data))
length(col(data))/17
setwd("~/Master 1/Semestre 2/Data Mining/Projet Bank Marketind Data/bases de données banques")
knitr::opts_chunk$set(echo = TRUE)
df<- read.csv("bank.csv", header = TRUE, sep = ";")
head(df)
str(df)
df<- read.csv("bank-full.csv", header = TRUE, sep = ";")
head(df)
str(df)
view(df)
df<- read.csv("bank-additional.csv", header = TRUE, sep = ";")
head(df)
str(df)
summary(df)
View(data)
className("character", data)
class("character", data)
type(data)
class(data)
class(data$age)
